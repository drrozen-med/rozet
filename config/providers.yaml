# Map logical orchestrator/worker roles to provider + model configuration.
# Copy to providers.yaml and edit values to match your credentials/quota.

orchestrator:
  provider: openai
  model: openai/gpt-4o-mini  # GPT-4o mini (affordable, fast, reliable) - UPDATED
  temperature: 0.0
  endpoint: https://openrouter.ai/api/v1  # Using OpenRouter for flexibility
  system_prompt_path: prompts/orchestrator.md
  # Alternative configurations:
  # - provider: ollama
  #   model: gpt-oss:20b  # Local gpt-oss-20b via Ollama (self-hosted, 16GB RAM)
  #   endpoint: http://localhost:11434
  # - provider: anthropic
  #   model: claude-3-5-sonnet-20241022
  # - provider: gemini
  #   model: gemini-2.0-flash-exp

# Workers disabled for now - using orchestrator only with GPT-5-nano
# workers:
#   cloud_fast:
#     provider: openai
#     model: gpt-5-nano
#     budget_usd: 0.05

# Define per-provider credential environment variables.
credentials:
  gemini: GEMINI_API_KEY  # Optional
  openai: OPENAI_API_KEY  # Required for orchestrator
  anthropic: ANTHROPIC_API_KEY  # Optional

# Optional: default budgets and limits.
budget:
  max_session_usd: 5.0
  max_task_usd: 0.75

# OpenCode integration: model filtering configuration
opencode:
  # Models to disable (override OpenCode's hardcoded blacklist)
  disabled_models:
    # - "gpt-5-chat-latest"  # Uncomment to disable specific models
  # If true, only show models in allowed_models (whitelist mode)
  allowed_models_only: false
  # Whitelist of allowed models (if allowed_models_only is true)
  allowed_models:
    # - "openai/gpt-5-nano"
    # - "anthropic/claude-3-5-sonnet"
  # Provider priority (order to try providers)
  provider_priority:
    - "openrouter"  # Try first
    - "openai"      # Fallback
    - "anthropic"   # Last resort

# Future work: add azure, aws, groq endpoints.
