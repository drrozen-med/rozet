# Rozet REPL Integration Plan

## Current State âœ…

### 1. LangChain Integration (YES, Comprehensive!)
âœ… **Context Management**: Uses `ConversationSummaryBufferMemory`  
âœ… **Task Planning**: Uses `BaseChatModel` from LangChain  
âœ… **Provider Factory**: Returns LangChain-compatible models  
âœ… **All LLM calls**: Go through LangChain abstractions  

**Why LangChain is Great Here:**
- âœ… Unified interface for all providers (OpenAI, Gemini, Ollama, etc.)
- âœ… Memory management built-in (summary buffers)
- âœ… Easy to switch providers without code changes
- âœ… Production-ready abstractions
- âœ… Consistent message format across providers

### 2. Model Switching (Already Supported!)
âœ… **Local Models**: Qwen via Ollama (`langchain-ollama`)  
âœ… **Cloud Models**: OpenAI, Gemini, Anthropic  
âœ… **Config-Driven**: Easy switching via `config/providers.yaml`  
âœ… **Custom Endpoints**: Support for OpenAI OSS-20, custom APIs  

### 3. Scaffolding/System Prompts (Already Supported!)
âœ… **Configurable**: `system_prompt_path` in config  
âœ… **Behavioral Framework**: Ready to integrate  
âœ… **Provider-Specific**: Each model can have its own prompt  

## Next Milestone: Rozet REPL Based on OpenCode

### Architecture Plan

```
Rozet REPL (based on OpenCode)
â”œâ”€â”€ Core: OpenCode's session/tool infrastructure
â”œâ”€â”€ REPL: Our conversational interface (orchestrator/tui.py)
â”œâ”€â”€ Orchestrator: Task planning & coordination
â”œâ”€â”€ Workers: Local (Qwen) + Cloud (GPT-5 nano/mini)
â””â”€â”€ Scaffolding: Behavioral framework system prompts
```

### Integration Steps

#### Step 1: Rozet Command in OpenCode âœ… (Partially Done)
```bash
opencode rozet repl          # Start REPL
opencode rozet plan "task"   # Quick plan
```

#### Step 2: Use OpenCode's Session System
- **OpenCode has**: Session management, message history, tool execution
- **We need**: Integrate our orchestrator into OpenCode's session flow
- **Benefit**: Get all OpenCode tools (read, write, bash, etc.) for free

#### Step 3: LangChain Integration Points
```python
# Current: Our REPL calls LangChain directly
orchestrator_llm.invoke(messages)

# Future: OpenCode session â†’ Our orchestrator â†’ LangChain â†’ Workers
opencode_session â†’ orchestrator.plan() â†’ langchain_model â†’ workers.execute()
```

#### Step 4: Model Routing
```yaml
# config/providers.yaml
orchestrator:
  provider: openai      # or ollama for local
  model: gpt-4o-mini    # or qwen2.5-coder:14b
  
workers:
  local:
    provider: ollama
    model: qwen2.5-coder:14b-instruct-q5_K_M
  cloud_fast:
    provider: openai
    model: gpt-4o-mini
```

### Current Code Structure

```
orchestrator/
â”œâ”€â”€ cli.py              âœ… CLI entry point
â”œâ”€â”€ tui.py              âœ… REPL implementation  
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ context_manager.py    âœ… LangChain memory
â”‚   â”œâ”€â”€ task_planner.py       âœ… LangChain LLM
â”‚   â””â”€â”€ coordinator.py        âœ… Task routing
â”œâ”€â”€ providers/
â”‚   â””â”€â”€ factory.py            âœ… LangChain model factory
â””â”€â”€ config_loader.py          âœ… Config management
```

### What We Need to Build

1. **OpenCode Integration Module**
   - Wrap OpenCode's session/tool system
   - Route to our orchestrator
   - Maintain REPL interface

2. **Session Bridge**
   - OpenCode session â†’ Our orchestrator context
   - Tool execution â†’ OpenCode tools
   - Message history â†’ LangChain memory

3. **Rozet Command**
   - `opencode rozet repl` - Start REPL
   - Integrate with OpenCode's TUI infrastructure (but keep REPL style)

### Benefits of LangChain Here

âœ… **Unified Model Interface**: Switch between Qwen/Ollama/OpenAI/Gemini seamlessly  
âœ… **Memory Management**: Automatic context summarization  
âœ… **Provider Abstraction**: Write once, works with any provider  
âœ… **Tool Integration**: LangChain tools can call OpenCode tools  
âœ… **Production Ready**: Battle-tested abstractions  

### Next Steps (UPDATED PRIORITY)

1. âœ… Confirm LangChain usage (DONE - we're using it comprehensively)
2. âœ… Model switching capability (DONE - config-driven in our orchestrator)
3. âœ… System prompt scaffolding (DONE - configurable)
4. ðŸ”¥ **CURRENT PRIORITY**: Fix OpenCode's model/provider system (see `OPENCODE_PROVIDER_FIX_PLAN.md`)
   - Create provider bridge with clean API
   - Replace hardcoded priorities with config-driven approach
   - Make model switching programmatically controllable
   - Testable with existing infrastructure
5. ðŸ”„ **Then**: Integrate REPL into OpenCode as "Rozet" command
6. ðŸ”„ **After**: Connect OpenCode session system to our orchestrator
7. ðŸ”„ **Finally**: TUI modifications (if still needed)

## Summary

**YES, we're using LangChain extensively!** âœ…
- Context management
- Task planning  
- Model abstraction
- Provider switching

**Model switching works!** âœ…
- Config-driven
- Local (Qwen/Ollama) + Cloud (OpenAI/Gemini)
- Easy to add OpenAI OSS-20

**System prompts ready!** âœ…
- Configurable per provider
- Behavioral framework ready

**Next: Integrate into OpenCode as "Rozet"** ðŸ”„

